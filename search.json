[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "This is a template example for lab journaling. Students in the data science courses at the Institute of Entrepreneurship will use this template to learn R for business analytics. Students can replace this text as they wish."
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "My Lab Journal",
    "section": "How to use",
    "text": "How to use\n\nAccept the assignment and get your own github repo.\nBlog/journal what you are doing in R, by editing the .qmd files.\nSee the links page for lots of helpful links on learning R.\nChange everything to make it your own.\nMake sure to render you website every time before you want to upload changes."
  },
  {
    "objectID": "content/01_journal/09_iv.html",
    "href": "content/01_journal/09_iv.html",
    "title": "Chap9: IV",
    "section": "",
    "text": "# Libraries\nlibrary(tidyverse)\n\n#&gt; ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#&gt; ✔ dplyr     1.1.4     ✔ readr     2.1.4\n#&gt; ✔ forcats   1.0.0     ✔ stringr   1.5.1\n#&gt; ✔ ggplot2   3.4.4     ✔ tibble    3.2.1\n#&gt; ✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n#&gt; ✔ purrr     1.0.2     \n#&gt; ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#&gt; ✖ dplyr::filter() masks stats::filter()\n#&gt; ✖ dplyr::lag()    masks stats::lag()\n#&gt; ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(dagitty)\nlibrary(ggdag)\n\n#&gt; \n#&gt; Attaching package: 'ggdag'\n#&gt; \n#&gt; The following object is masked from 'package:stats':\n#&gt; \n#&gt;     filter\n\nlibrary(estimatr)\nlibrary(AER)\n\n#&gt; Loading required package: car\n#&gt; Loading required package: carData\n#&gt; \n#&gt; Attaching package: 'car'\n#&gt; \n#&gt; The following object is masked from 'package:dplyr':\n#&gt; \n#&gt;     recode\n#&gt; \n#&gt; The following object is masked from 'package:purrr':\n#&gt; \n#&gt;     some\n#&gt; \n#&gt; Loading required package: lmtest\n#&gt; Loading required package: zoo\n#&gt; \n#&gt; Attaching package: 'zoo'\n#&gt; \n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     as.Date, as.Date.numeric\n#&gt; \n#&gt; Loading required package: sandwich\n#&gt; Loading required package: survival\n\n#1\n# Definition of the DAG\ndag &lt;- dagify(\n  spentTime ~ featureUsed,\n  spentTime ~ Unobserved,\n  featureUsed ~ Unobserved,\n  featureUsed ~ encourgement,\n  exposure = \"featureUsed\",\n  latent = \"Unobserved\",\n  outcome = \"spentTime\",\n  coords = list(x = c(Unobserved = 1, featureUsed = 0, spentTime = 2, encourgement = -1),\n                y = c(Unobserved = 1, featureUsed = 0, spentTime = 0, encourgement = 0)),\n  labels = c(\n    \"spentTime\" = \"Time Spent on the App\",\n    \"featureUsed\" = \"The new feature is used\",\n    \"encourgement\" = \"User encourgement to use feature\",\n    \"Unobserved\" = \"Unobserved variables\"\n  )\n)\n# Plot DAG\nggdag(dag, text = FALSE, use_labels = \"label\")\n\n\n\n\n\n\n\n########################################################\n\n# Load the data\ndf &lt;- readRDS('C:/Users/user/OneDrive/Desktop/UNI/TUHH/SEM 3/CAUSAL DATA SC/RDS/rand_enc.rds')\n\n# Explore the data\nhead(df)\n\n\n\n  \n\n\n# 2\n# Naive Estimate\nnaive_estimate &lt;- lm(time_spent ~ used_ftr, data = df)\nsummary(naive_estimate)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = time_spent ~ used_ftr, data = df)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept) 18.86993    0.06955   271.3   &lt;2e-16 ***\n#&gt; used_ftr    10.82269    0.10888    99.4   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 5.351 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.497,  Adjusted R-squared:  0.497 \n#&gt; F-statistic:  9881 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n########################################################\n\n# 3\n# The correlation matrix\ncor(df) %&gt;% round(2)\n\n#&gt;            rand_enc used_ftr time_spent\n#&gt; rand_enc       1.00     0.20       0.13\n#&gt; used_ftr       0.20     1.00       0.71\n#&gt; time_spent     0.13     0.71       1.00\n\ncat(\"  ## Since the naive estimate (10.82269) is greater than the IV robust estimate using rand_enc as \n  ## an instrument (9.738175), we would consider the naive estimate to have an upward bias. This \n  ## implies that the naive estimate overestimates the effect of used_ftr on time_spent.\")\n\n#&gt;   ## Since the naive estimate (10.82269) is greater than the IV robust estimate using rand_enc as \n#&gt;   ## an instrument (9.738175), we would consider the naive estimate to have an upward bias. This \n#&gt;   ## implies that the naive estimate overestimates the effect of used_ftr on time_spent.\n\ncat(\"## A strong correlation between used_ftr and time_spent can be seen.\")\n\n#&gt; ## A strong correlation between used_ftr and time_spent can be seen.\n\ncat(\"   ## Assuming rand_enc as an Instrumental variable would make sense because\n  ## it has a weak correlation with the outcome (time_spent) and a stronger\n  ## one with the treatment (used_ftr).\")\n\n#&gt;    ## Assuming rand_enc as an Instrumental variable would make sense because\n#&gt;   ## it has a weak correlation with the outcome (time_spent) and a stronger\n#&gt;   ## one with the treatment (used_ftr).\n\ncat(\"## While the correlation between the instrumental variable and outcome is not \n  ## zero (maybe due to noise), this correlation is relatively low.\")\n\n#&gt; ## While the correlation between the instrumental variable and outcome is not \n#&gt;   ## zero (maybe due to noise), this correlation is relatively low.\n\n########################################################\n\n# 4\n# Instrumental Variable Estimation with rand_enc and robust standard errors\n\nmodel_iv_robust &lt;- iv_robust(time_spent ~ used_ftr | rand_enc, data = df)\nsummary(model_iv_robust)\n\n#&gt; \n#&gt; Call:\n#&gt; iv_robust(formula = time_spent ~ used_ftr | rand_enc, data = df)\n#&gt; \n#&gt; Standard error type:  HC2 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value  Pr(&gt;|t|) CI Lower CI Upper   DF\n#&gt; (Intercept)   19.312     0.2248   85.89 0.000e+00   18.872    19.75 9998\n#&gt; used_ftr       9.738     0.5353   18.19 8.716e-73    8.689    10.79 9998\n#&gt; \n#&gt; Multiple R-squared:  0.4921 ,    Adjusted R-squared:  0.492 \n#&gt; F-statistic:   331 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n## Hansen J test\n\n#Residuals and fitted values from the model\nresiduals_iv &lt;- residuals(model_iv_robust)\nfitted_values_iv &lt;- fitted(model_iv_robust)\n\n# Perform Hansen J test\nhansen_test_stat &lt;- sum(residuals_iv * fitted_values_iv)\np_value_hansen &lt;- 1 - pchisq(hansen_test_stat, df = 1)\n\n# Results\ncat(\"Hansen J Test Statistic:\", hansen_test_stat, \"\\n\")\n\n#&gt; Hansen J Test Statistic: 0\n\ncat(\"P-value:\", p_value_hansen, \"\\n\")\n\n#&gt; P-value: 1\n\ncat(\"  ## Hansen J test with a test statistic close to 0 and a p-value close to 1 indicates\n  ## that the instrument used in the model is not violating the over-identifying\n  ## restrictions. In other words, the instrument is valid for the model, and there is \n  ## no evidence to suggest that the instrument is endogenous or correlated with the error term.\") \n\n#&gt;   ## Hansen J test with a test statistic close to 0 and a p-value close to 1 indicates\n#&gt;   ## that the instrument used in the model is not violating the over-identifying\n#&gt;   ## restrictions. In other words, the instrument is valid for the model, and there is \n#&gt;   ## no evidence to suggest that the instrument is endogenous or correlated with the error term.\n\ncat(\"Naive Estimate:\", coef(naive_estimate)['used_ftr'], \"\\n\")\n\n#&gt; Naive Estimate: 10.82269\n\ncat(\"IV Robust Estimate (rand_enc):\", model_iv_robust$coefficients['used_ftr'], \"\\n\")\n\n#&gt; IV Robust Estimate (rand_enc): 9.738175\n\ncat(\"  ## Since the naive estimate (10.82269) is greater than the IV robust estimate using rand_enc as\n  ## an instrument (9.738175), we would consider the naive estimate to have an upward bias. This \n  ## implies that the naive estimate overestimates the effect of used_ftr on time_spent.\")\n\n#&gt;   ## Since the naive estimate (10.82269) is greater than the IV robust estimate using rand_enc as\n#&gt;   ## an instrument (9.738175), we would consider the naive estimate to have an upward bias. This \n#&gt;   ## implies that the naive estimate overestimates the effect of used_ftr on time_spent."
  },
  {
    "objectID": "content/01_journal/07_matching.html",
    "href": "content/01_journal/07_matching.html",
    "title": "Chap7: Matching",
    "section": "",
    "text": "# Libraries\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\nlibrary(MatchIt)\nlibrary(dplyr)\n\n# Data\ncustomer_data &lt;- readRDS(\"C:/Users/user/OneDrive/Desktop/UNI/TUHH/SEM 3/CAUSAL DATA SC/RDS/membership.rds\")\n\n# Exploratory Data Analysis\nsummary(customer_data)\n\n#&gt;       age             sex         pre_avg_purch         card       \n#&gt;  Min.   :16.00   Min.   :0.0000   Min.   :-14.23   Min.   :0.0000  \n#&gt;  1st Qu.:29.80   1st Qu.:0.0000   1st Qu.: 51.82   1st Qu.:0.0000  \n#&gt;  Median :38.80   Median :1.0000   Median : 70.15   Median :0.0000  \n#&gt;  Mean   :40.37   Mean   :0.5038   Mean   : 70.42   Mean   :0.4232  \n#&gt;  3rd Qu.:49.20   3rd Qu.:1.0000   3rd Qu.: 88.79   3rd Qu.:1.0000  \n#&gt;  Max.   :90.00   Max.   :1.0000   Max.   :169.42   Max.   :1.0000  \n#&gt;    avg_purch     \n#&gt;  Min.   :-28.61  \n#&gt;  1st Qu.: 54.02  \n#&gt;  Median : 76.24  \n#&gt;  Mean   : 76.61  \n#&gt;  3rd Qu.: 98.54  \n#&gt;  Max.   :192.91\n\ncor(customer_data)\n\n#&gt;                      age          sex pre_avg_purch        card   avg_purch\n#&gt; age           1.00000000  0.012532675   0.517506430 0.105533628 0.448632638\n#&gt; sex           0.01253267  1.000000000  -0.001221386 0.008468092 0.002181853\n#&gt; pre_avg_purch 0.51750643 -0.001221386   1.000000000 0.192333327 0.855828507\n#&gt; card          0.10553363  0.008468092   0.192333327 1.000000000 0.382352233\n#&gt; avg_purch     0.44863264  0.002181853   0.855828507 0.382352233 1.000000000\n\n# Directed Acyclic Graph (DAG) using dagitty\ncollider &lt;- dagitty('dag {\n  avg_purch &lt;- plus_membership\n  avg_purch &lt;- age\n  avg_purch &lt;- sex\n  avg_purch &lt;- pre_avg_purch\n}')\n\n# Plot DAG using ggdag\nggdag(collider) +\n  geom_dag_point() +\n  geom_dag_text(color = \"green\") +\n  geom_dag_edges(edge_color = \"brown\")\n\n\n\n\n\n\n\n# Naive Estimation\nmodel_naive &lt;- lm(avg_purch ~ card, data = customer_data)\nsummary(model_naive)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = customer_data)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -101.515  -20.684   -0.199   20.424  120.166 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  65.9397     0.3965  166.29   &lt;2e-16 ***\n#&gt; card         25.2195     0.6095   41.38   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.11 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.1462, Adjusted R-squared:  0.1461 \n#&gt; F-statistic:  1712 on 1 and 9998 DF,  p-value: &lt; 2.2e-16\n\n# Coarsened Exact Matching (CEM)\ncem &lt;- matchit(card ~ age + pre_avg_purch + sex,\n               data = customer_data, \n               method = 'cem', \n               estimand = 'ATE')\n\n# Covariate Balance after Matching\nsummary(cem)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch + sex, data = customer_data, \n#&gt;     method = \"cem\", estimand = \"ATE\")\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2136     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3962     1.0276    0.1092\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; sex             0.0086\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 40.1743       40.1557          0.0014     0.9993    0.0016\n#&gt; pre_avg_purch       70.4611       70.0938          0.0141     0.9929    0.0044\n#&gt; sex                  0.5040        0.5040          0.0000          .    0.0000\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0064          0.1222\n#&gt; pre_avg_purch   0.0130          0.1558\n#&gt; sex             0.0000          0.0000\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 5429.65    3844\n#&gt; Matched       5716.      4164\n#&gt; Unmatched       52.        68\n#&gt; Discarded        0.         0\n\n# Matched data\ndf_cem &lt;- match.data(cem)\n\n# Using matched data\nmodel_cem &lt;- lm(avg_purch ~ card, data = df_cem, weights = weights)\nsummary(model_cem)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_cem, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -159.349  -20.459   -0.151   19.863  161.528 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  69.9896     0.3984  175.66   &lt;2e-16 ***\n#&gt; card         15.2043     0.6137   24.77   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.12 on 9878 degrees of freedom\n#&gt; Multiple R-squared:  0.0585, Adjusted R-squared:  0.0584 \n#&gt; F-statistic: 613.7 on 1 and 9878 DF,  p-value: &lt; 2.2e-16\n\n# Nearest Neighbor Matching (NN)\nnn &lt;- matchit(card ~ age + pre_avg_purch + sex,\n              data = customer_data,\n              method = \"nearest\",\n              distance = \"mahalanobis\",\n              replace = TRUE)\n\n# Covariate Balance after Matching\nsummary(nn)\n\n#&gt; \n#&gt; Call:\n#&gt; matchit(formula = card ~ age + pre_avg_purch + sex, data = customer_data, \n#&gt;     method = \"nearest\", distance = \"mahalanobis\", replace = TRUE)\n#&gt; \n#&gt; Summary of Balance for All Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       39.1574          0.2064     1.1524    0.0438\n#&gt; pre_avg_purch       76.3938       66.0438          0.3936     1.0276    0.1092\n#&gt; sex                  0.5087        0.5002          0.0171          .    0.0086\n#&gt;               eCDF Max\n#&gt; age             0.0864\n#&gt; pre_avg_purch   0.1545\n#&gt; sex             0.0086\n#&gt; \n#&gt; Summary of Balance for Matched Data:\n#&gt;               Means Treated Means Control Std. Mean Diff. Var. Ratio eCDF Mean\n#&gt; age                 42.0331       41.9964          0.0026     1.0171    0.0014\n#&gt; pre_avg_purch       76.3938       76.2937          0.0038     1.0178    0.0012\n#&gt; sex                  0.5087        0.5087          0.0000          .    0.0000\n#&gt;               eCDF Max Std. Pair Dist.\n#&gt; age             0.0061          0.0281\n#&gt; pre_avg_purch   0.0076          0.0301\n#&gt; sex             0.0000          0.0000\n#&gt; \n#&gt; Sample Sizes:\n#&gt;               Control Treated\n#&gt; All           5768.      4232\n#&gt; Matched (ESS) 1992.19    4232\n#&gt; Matched       2677.      4232\n#&gt; Unmatched     3091.         0\n#&gt; Discarded        0.         0\n\n# Matched data\ndf_nn &lt;- match.data(nn)\n\n# Estimation using matched data\nmodel_nn &lt;- lm(avg_purch ~ card, data = df_nn, weights = weights)\nsummary(model_nn)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_nn, weights = weights)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -132.730  -21.288   -1.675   18.318  146.631 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  76.5634     0.5881  130.19   &lt;2e-16 ***\n#&gt; card         14.5957     0.7514   19.42   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 30.43 on 6907 degrees of freedom\n#&gt; Multiple R-squared:  0.05179,    Adjusted R-squared:  0.05166 \n#&gt; F-statistic: 377.3 on 1 and 6907 DF,  p-value: &lt; 2.2e-16\n\n# Propensity Score Matching (PSM)\nmodel_prop &lt;- glm(card ~ age + pre_avg_purch + sex,\n                  data = customer_data,\n                  family = binomial(link = \"logit\"))\nsummary(model_prop)\n\n#&gt; \n#&gt; Call:\n#&gt; glm(formula = card ~ age + pre_avg_purch + sex, family = binomial(link = \"logit\"), \n#&gt;     data = customer_data)\n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error z value Pr(&gt;|z|)    \n#&gt; (Intercept)   -1.4298676  0.0752043 -19.013   &lt;2e-16 ***\n#&gt; age            0.0011486  0.0017761   0.647    0.518    \n#&gt; pre_avg_purch  0.0148262  0.0009264  16.003   &lt;2e-16 ***\n#&gt; sex            0.0359388  0.0412622   0.871    0.384    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; (Dispersion parameter for binomial family taken to be 1)\n#&gt; \n#&gt;     Null deviance: 13626  on 9999  degrees of freedom\n#&gt; Residual deviance: 13249  on 9996  degrees of freedom\n#&gt; AIC: 13257\n#&gt; \n#&gt; Number of Fisher Scoring iterations: 4\n\n# Propensities to the table\ndf_aug &lt;- customer_data %&gt;% mutate(propensity = predict(model_prop, type = \"response\"))\n\n# Data by Inverse Probability Weights (IPW) scores\ndf_ipw &lt;- df_aug %&gt;% mutate(ipw = (card / propensity) + ((1 - card) / (1 - propensity)))\n\n# Data with IPW scores\ndf_ipw %&gt;% select(card, age, pre_avg_purch, sex, propensity, ipw)\n\n\n\n  \n\n\n# Estimation using IPW scores\nmodel_ipw &lt;- lm(avg_purch ~ card, data = df_ipw, weights = ipw)\nsummary(model_ipw)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = avg_purch ~ card, data = df_ipw, weights = ipw)\n#&gt; \n#&gt; Weighted Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -205.353  -28.995   -0.275   28.787  214.307 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  70.2628     0.4320  162.66   &lt;2e-16 ***\n#&gt; card         14.9573     0.6109   24.48   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 43.19 on 9998 degrees of freedom\n#&gt; Multiple R-squared:  0.05657,    Adjusted R-squared:  0.05647 \n#&gt; F-statistic: 599.5 on 1 and 9998 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/05_dag.html",
    "href": "content/01_journal/05_dag.html",
    "title": "Chap5: DAG",
    "section": "",
    "text": "# Libraries\nlibrary(tidyverse)\nlibrary(dagitty)\nlibrary(ggdag)\n\n# Directed Acyclic Graph (DAG) for a Sales Example\nsales_dag &lt;- dagify(\n  sales ~ parking_spots,\n  sales ~ location,\n  parking_spots ~ location, \n  labels = c(\n    \"sales\" = \"Sales\",\n    \"parking_spots\" = \"Parking \\n Spots\",\n    \"location\" = \"Location\"\n  )\n)\nggdag(sales_dag, use_labels = \"label\", text = FALSE)\n\n\n\n\n\n\n\n######################################################\n\n# Customer Satisfaction Data\ncustomer_satisfaction_data &lt;- readRDS('C:/Users/user/OneDrive/Desktop/UNI/TUHH/SEM 3/CAUSAL DATA SC/RDS/customer_sat.rds')\n\n# The Dataset\nhead(customer_satisfaction_data)\n\n\n\n  \n\n\n# Regression Analysis on Satisfaction and Follow-ups\n## Simple Linear Regression ##\nmodel_simple &lt;- lm(satisfaction ~ follow_ups, data = customer_satisfaction_data)\nsummary(model_simple)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups, data = customer_satisfaction_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -12.412  -5.257   1.733   4.506  12.588 \n#&gt; \n#&gt; Coefficients:\n#&gt;             Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)  78.8860     4.2717  18.467 1.04e-10 ***\n#&gt; follow_ups   -3.3093     0.6618  -5.001 0.000243 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 7.923 on 13 degrees of freedom\n#&gt; Multiple R-squared:  0.658,  Adjusted R-squared:  0.6316 \n#&gt; F-statistic: 25.01 on 1 and 13 DF,  p-value: 0.0002427\n\n# Multiple Linear Regression for Subscription\nmodel_multiple &lt;- lm(satisfaction ~ follow_ups + subscription, data = customer_satisfaction_data)\nsummary(model_multiple)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satisfaction ~ follow_ups + subscription, data = customer_satisfaction_data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -4.3222 -2.1972  0.3167  2.2667  3.9944 \n#&gt; \n#&gt; Coefficients:\n#&gt;                      Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)           26.7667     6.6804   4.007  0.00206 ** \n#&gt; follow_ups             2.1944     0.7795   2.815  0.01682 *  \n#&gt; subscriptionPremium   44.7222     5.6213   7.956 6.88e-06 ***\n#&gt; subscriptionPremium+  18.0722     2.1659   8.344 4.37e-06 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 2.958 on 11 degrees of freedom\n#&gt; Multiple R-squared:  0.9597, Adjusted R-squared:  0.9487 \n#&gt; F-statistic: 87.21 on 3 and 11 DF,  p-value: 5.956e-08\n\n# Coefficients from the Two Models\ncoef_comparison &lt;- data.frame(\n  Model = c(\"Model 1\", \"Model 2\"),\n  Intercept = c(coef(model_simple)[1], coef(model_multiple)[1]),\n  FollowUps = c(coef(model_simple)[2], coef(model_multiple)[2]),\n  PremiumPlus = c(0, coef(model_multiple)[grep(\"subscriptionPremium\\\\+\", names(coef(model_multiple)))]),\n  Elite = c(0, coef(model_multiple)[grep(\"subscriptionElite\", names(coef(model_multiple)))])\n)\nprint(coef_comparison)\n\n#&gt;                        Model Intercept FollowUps PremiumPlus Elite\n#&gt;                      Model 1  78.88605 -3.309302     0.00000     0\n#&gt; subscriptionPremium+ Model 2  26.76667  2.194444    18.07222     0\n\n## Model 2 has a lower baseline satisfaction due to a reduced intercept compared to Model 1.\n## Accounting for subscription changes the direction of the relationship between Follow-ups and satisfaction,\n## turning it from negative to positive.\n## There is a positive impact on satisfaction for \"PremiumPlus,\" while there is no additional impact on the \"Elite\" subscription level.\n\n# Visualization of the Data\n### Simpson's Paradox: Subscription as a Confounding Factor ###\n\n## Without conditioning on subscription\nsimps_not_cond &lt;- ggplot(customer_satisfaction_data, aes(x = follow_ups, y = satisfaction)) +\n  geom_point(alpha = 0.8) +\n  stat_smooth(method = \"lm\", se = F) +\n  labs(title = \"Relationship between Follow-ups and Satisfaction\",\n       x = \"Follow-ups\",\n       y = \"Satisfaction\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n## Conditioning on subscription\nsimps_cond &lt;- ggplot(customer_satisfaction_data, aes(x = follow_ups, y = satisfaction, color = subscription)) +\n  geom_point(alpha = 0.8) +\n  stat_smooth(method = \"lm\", se = F, size = 1) +\n  labs(title = \"Relationship between Follow-ups and Satisfaction by Subscription Level\",\n       x = \"Follow-ups\",\n       y = \"Satisfaction\",\n       color = \"Subscription\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\n#&gt; Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#&gt; ℹ Please use `linewidth` instead.\n\nsimps_not_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\nsimps_cond\n\n#&gt; `geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "content/01_journal/03_regression.html",
    "href": "content/01_journal/03_regression.html",
    "title": "Chap3: Regression",
    "section": "",
    "text": "# Libraries\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nlibrary(modelr)\n\n# The dataset containing information on car prices\ncar_dataset &lt;- readRDS(\"C:/Users/user/OneDrive/Desktop/UNI/TUHH/SEM 3/CAUSAL DATA SC/RDS/car_prices.rds\")\n\n# The dataset and its dimensions\ndata_dimensions &lt;- dim(car_dataset)\nnum_rows &lt;- data_dimensions[1]\nnum_columns &lt;- data_dimensions[2]\ncat(\"Total rows in the dataset:\", num_rows, \"\\n\")\n\n#&gt; Total rows in the dataset: 181\n\ncat(\"Total columns in the dataset:\", num_columns, \"\\n\")\n\n#&gt; Total columns in the dataset: 22\n\n# Column names for each dataframe in the dataset\ncolumn_names_list &lt;- lapply(car_dataset, colnames)\n\n# Explore the data types of selected columns\nfirst_car_width &lt;- head(car_dataset$carwidth, 1)\ncat(\"The data type of car width is\", typeof(first_car_width), \"\\n\")\n\n#&gt; The data type of car width is double\n\nfirst_car_body &lt;- head(car_dataset$carbody, 1)\ncat(\"The data type of car body is\", typeof(first_car_body), \"\\n\")\n\n#&gt; The data type of car body is character\n\nprint(\"Two main types are char and double.\")\n\n#&gt; [1] \"Two main types are char and double.\"\n\nprint(\"Numeric types like double are suitable for quantitative information, while strings and char are useful for textual and categorical information. It is crucial to accurately represent real-world attributes in the dataset.\")\n\n#&gt; [1] \"Numeric types like double are suitable for quantitative information, while strings and char are useful for textual and categorical information. It is crucial to accurately represent real-world attributes in the dataset.\"\n\n# Linear Regression\nglimpse(car_dataset)\n\n#&gt; Rows: 181\n#&gt; Columns: 22\n#&gt; $ aspiration       &lt;chr&gt; \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std\", \"std…\n#&gt; $ doornumber       &lt;chr&gt; \"two\", \"two\", \"two\", \"four\", \"four\", \"two\", \"four\", \"…\n#&gt; $ carbody          &lt;chr&gt; \"convertible\", \"convertible\", \"hatchback\", \"sedan\", \"…\n#&gt; $ drivewheel       &lt;chr&gt; \"rwd\", \"rwd\", \"rwd\", \"fwd\", \"4wd\", \"fwd\", \"fwd\", \"fwd…\n#&gt; $ enginelocation   &lt;chr&gt; \"front\", \"front\", \"front\", \"front\", \"front\", \"front\",…\n#&gt; $ wheelbase        &lt;dbl&gt; 88.6, 88.6, 94.5, 99.8, 99.4, 99.8, 105.8, 105.8, 105…\n#&gt; $ carlength        &lt;dbl&gt; 168.8, 168.8, 171.2, 176.6, 176.6, 177.3, 192.7, 192.…\n#&gt; $ carwidth         &lt;dbl&gt; 64.1, 64.1, 65.5, 66.2, 66.4, 66.3, 71.4, 71.4, 71.4,…\n#&gt; $ carheight        &lt;dbl&gt; 48.8, 48.8, 52.4, 54.3, 54.3, 53.1, 55.7, 55.7, 55.9,…\n#&gt; $ curbweight       &lt;dbl&gt; 2548, 2548, 2823, 2337, 2824, 2507, 2844, 2954, 3086,…\n#&gt; $ enginetype       &lt;chr&gt; \"dohc\", \"dohc\", \"ohcv\", \"ohc\", \"ohc\", \"ohc\", \"ohc\", \"…\n#&gt; $ cylindernumber   &lt;chr&gt; \"four\", \"four\", \"six\", \"four\", \"five\", \"five\", \"five\"…\n#&gt; $ enginesize       &lt;dbl&gt; 130, 130, 152, 109, 136, 136, 136, 136, 131, 131, 108…\n#&gt; $ fuelsystem       &lt;chr&gt; \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi\", \"mpfi…\n#&gt; $ boreratio        &lt;dbl&gt; 3.47, 3.47, 2.68, 3.19, 3.19, 3.19, 3.19, 3.19, 3.13,…\n#&gt; $ stroke           &lt;dbl&gt; 2.68, 2.68, 3.47, 3.40, 3.40, 3.40, 3.40, 3.40, 3.40,…\n#&gt; $ compressionratio &lt;dbl&gt; 9.00, 9.00, 9.00, 10.00, 8.00, 8.50, 8.50, 8.50, 8.30…\n#&gt; $ horsepower       &lt;dbl&gt; 111, 111, 154, 102, 115, 110, 110, 110, 140, 160, 101…\n#&gt; $ peakrpm          &lt;dbl&gt; 5000, 5000, 5000, 5500, 5500, 5500, 5500, 5500, 5500,…\n#&gt; $ citympg          &lt;dbl&gt; 21, 21, 19, 24, 18, 19, 19, 19, 17, 16, 23, 23, 21, 2…\n#&gt; $ highwaympg       &lt;dbl&gt; 27, 27, 26, 30, 22, 25, 25, 25, 20, 22, 29, 29, 28, 2…\n#&gt; $ price            &lt;dbl&gt; 13495.00, 16500.00, 16500.00, 13950.00, 17450.00, 152…\n\n# Convert all columns to numeric\ncar_dataset[] &lt;- lapply(car_dataset, as.numeric)\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n#&gt; Warning in lapply(car_dataset, as.numeric): NAs introduced by coercion\n\n# The correlation matrix\ncorrelation_matrix &lt;- car_dataset %&gt;% cor() %&gt;% round(2) %&gt;% Matrix::tril()\n\n# Data type and range of the chosen predictor (enginesize)\nfirst_enginesize &lt;- head(car_dataset$enginesize, 1)\nmin_enginesize &lt;- min(car_dataset$enginesize)\nmax_enginesize &lt;- max(car_dataset$enginesize)\ncat(\"The selected predictor is enginesize with a data type of \", typeof(first_enginesize), \" and it ranges between \", min_enginesize, \" and \", max_enginesize, \"\\n\")\n\n#&gt; The selected predictor is enginesize with a data type of  double  and it ranges between  61  and  326\n\n# The impact of enginesize on price\ncat(\"Enginesize has a significant effect on price, showing a positive correlation of 89%. As enginesize increases, the price tends to increase proportionally.\\n\")\n\n#&gt; Enginesize has a significant effect on price, showing a positive correlation of 89%. As enginesize increases, the price tends to increase proportionally.\n\n# New variable 'seat_heating_TRUE' with random TRUE/FALSE values\ndataset_with_seat_heating &lt;- car_dataset %&gt;%\n  mutate(seat_heating_TRUE = sample(c(TRUE, FALSE), size = nrow(car_dataset), replace = TRUE))\n\n# The distribution of seat_heating_TRUE\ntable(dataset_with_seat_heating$seat_heating_TRUE)\n\n#&gt; \n#&gt; FALSE  TRUE \n#&gt;    90    91\n\n# The regression with seat heating as a predictor\nlinear_regression_model &lt;- lm(price ~ seat_heating_TRUE, data = dataset_with_seat_heating)\n\n# The summary of the linear regression model\nsummary(linear_regression_model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = price ~ seat_heating_TRUE, data = dataset_with_seat_heating)\n#&gt; \n#&gt; Residuals:\n#&gt;    Min     1Q Median     3Q    Max \n#&gt;  -8940  -5260  -3004   2500  31342 \n#&gt; \n#&gt; Coefficients:\n#&gt;                       Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)            14058.3      845.5  16.627   &lt;2e-16 ***\n#&gt; seat_heating_TRUETRUE  -2106.2     1192.4  -1.766    0.079 .  \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 8021 on 179 degrees of freedom\n#&gt; Multiple R-squared:  0.01713,    Adjusted R-squared:  0.01164 \n#&gt; F-statistic:  3.12 on 1 and 179 DF,  p-value: 0.07904\n\ncat(\"Assigning all values as true for seat heating can lead to issues in regression analysis, including collinearity problems and difficulties in estimating coefficients.\\n\")\n\n#&gt; Assigning all values as true for seat heating can lead to issues in regression analysis, including collinearity problems and difficulties in estimating coefficients."
  },
  {
    "objectID": "content/01_journal/01_probability.html",
    "href": "content/01_journal/01_probability.html",
    "title": "Chap1: Probability",
    "section": "",
    "text": "Part 1.1\n\n# Given probabilities\nP_S &lt;- 0.3\nP_T_S &lt;- 0.2\nP_T_S_Bar &lt;- 0.6\nP_S_Bar &lt;- 1 - P_S\nP_T_Bar_S &lt;- 1 - P_T_S\nP_T_Bar_S_Bar &lt;- 1 - P_T_S_Bar\n\n# The conditional probabilities are:\nP_T_inter_S &lt;- P_S * P_T_S\nP_T_inter_S_Bar &lt;- P_S_Bar * P_T_S_Bar\nP_T_Bar_inter_S &lt;- P_S * P_T_Bar_S\nP_T_Bar_inter_S_Bar &lt;- P_S_Bar * P_T_Bar_S_Bar\n\n# The results to be printed:\nprint(P_T_inter_S)\n\n#&gt; [1] 0.06\n\nprint(P_T_inter_S_Bar)\n\n#&gt; [1] 0.42\n\nprint(P_T_Bar_inter_S)\n\n#&gt; [1] 0.24\n\nprint(P_T_Bar_inter_S_Bar)\n\n#&gt; [1] 0.28\n\n\n\n\nPart 1.2\n\n# Libraries\nlibrary(tidyverse)\nlibrary(ggVennDiagram)\n\n# Number of observations\nn &lt;- 1000\n\n# Tibble with user information:\napp_usage &lt;- tibble(\n  user_id = 1:n,\n  smartphone = rbinom(n, 1, 0.4),\n  tablet = ifelse(smartphone == 1, rbinom(n, 1, 0.2), rbinom(n, 1, 0.5)),\n  computer = ifelse(tablet == 1, rbinom(n, 1, 0.1), rbinom(n, 1, 0.3))\n)\n\n#At least one device is used per user:\napp_usage &lt;- app_usage %&gt;%\n  rowwise() %&gt;% \n  mutate(smartphone = ifelse(sum(smartphone, tablet, computer) == 0, 1, smartphone))\n\n# the first ten rows of the data:\nhead(app_usage, 10)\n\n\n\n  \n\n\n# column sums:\ncolSums(app_usage)\n\n#&gt;    user_id smartphone     tablet   computer \n#&gt;     500500        613        387        214\n\n# sets of phone, tablet, and computer users:\nset_phon &lt;- which(app_usage$smartphone == 1)\nset_tabl &lt;- which(app_usage$tablet == 1)\nset_comp &lt;- which(app_usage$computer == 1)\n\n# List of all sets:\nsets_all &lt;- list(set_phon, set_tabl, set_comp)\n\n# Plot Venn diagram:\nggVennDiagram(sets_all, category.names = c(\"Smartphone\", \"Tablet\", \"Computer\"),\n              label_percent_digit = 2) +\n  theme(legend.position = \"none\", \n        panel.background = element_rect(\"green\"),\n        strip.background = element_rect(\"red\")) +\n  scale_x_continuous(expand = expansion(mult = .24))\n\n\n\n\n\n\n\n# the intersection of sets (users using all three devices):\nset_all_three &lt;- Reduce(intersect, sets_all)\npercentage_all_three &lt;- length(set_all_three) / n * 100\ncat(\"Percentage of customers using all three devices:\", round(percentage_all_three, 2), \"%\\n\")\n\n#&gt; Percentage of customers using all three devices: 0.7 %\n\n# the union of sets (users using at least two devices):\nset_at_least_two &lt;- union(intersect(set_phon, set_tabl), union(intersect(set_phon, set_comp), intersect(set_comp, set_tabl)))\npercentage_at_least_two &lt;- length(set_at_least_two) / n * 100\ncat(\"Percentage of customers using at least two devices:\", round(percentage_at_least_two, 2), \"%\\n\")\n\n#&gt; Percentage of customers using at least two devices: 20.7 %\n\n# the sets of users using each device exclusively\nset_only_phon &lt;- set_phon[!(set_phon %in% set_tabl) & !(set_phon %in% set_comp)]\nset_only_tabl &lt;- set_tabl[!(set_tabl %in% set_phon) & !(set_tabl %in% set_comp)]\nset_only_comp &lt;- set_comp[!(set_comp %in% set_phon) & !(set_comp %in% set_tabl)]\npercentage_only_one &lt;- (length(set_only_phon) + length(set_only_tabl) + length(set_only_comp)) / n * 100\ncat(\"Percentage of customers using only one device:\", round(percentage_only_one, 2), \"%\\n\")\n\n#&gt; Percentage of customers using only one device: 79.3 %\n\n\n\n\nPart 1.3\n\n# Given probabilities\nprob_A &lt;- 0.04\nprob_B_given_A &lt;- 0.97\nprob_not_B_given_A &lt;- 0.01\nprob_not_A &lt;- 1 - prob_A\nprob_not_B_given_not_A &lt;- 1 - prob_B_given_A\nprob_not_B_given_not_A_not &lt;- 1 - prob_not_B_given_A\n\n# overall probability of B\nprob_B &lt;- (prob_B_given_A * prob_A) + (prob_not_B_given_A * prob_not_A)\n\n# conditional probability of not A given B\nprob_not_A_given_B &lt;- (prob_not_B_given_A * prob_not_A) / prob_B\n\n# conditional probability of A given B\nprob_A_given_B &lt;- (prob_B_given_A * prob_A) / prob_B\n\n# results\nprint(prob_not_A_given_B)\n\n#&gt; [1] 0.1983471\n\nprint(prob_A_given_B)\n\n#&gt; [1] 0.8016529"
  },
  {
    "objectID": "content/01_journal/02_statistics.html",
    "href": "content/01_journal/02_statistics.html",
    "title": "Chap2:Statistics",
    "section": "",
    "text": "random_vars &lt;- readRDS(\"C:/Users/user/OneDrive/Desktop/UNI/TUHH/SEM 3/CAUSAL DATA SC/RDS/random_vars.rds\")\nView(random_vars)\nage_var &lt;- random_vars$age\nincome_var &lt;- random_vars$income\n\n# The Expected Value\nexpected_value_age &lt;- sum(age_var) / length(age_var)\nexpected_value_income &lt;- sum(income_var) / length(income_var)\ncat(\"Expected age Value:\", expected_value_age, \"\\n\")\n\n#&gt; Expected age Value: 33.471\n\ncat(\"Expected income Value:\", expected_value_income, \"\\n\")\n\n#&gt; Expected income Value: 3510.731\n\n# The Variance\nmean_value_age &lt;- mean(age_var)\nmean_value_income &lt;- mean(income_var)\nvariance_value_age &lt;- sum((age_var - mean_value_age)^2) / (length(age_var) - 1)\nvariance_value_income&lt;- sum((income_var - mean_value_income)^2) / (length(income_var) - 1)\ncat(\"Variance age Value:\", variance_value_age, \"\\n\")\n\n#&gt; Variance age Value: 340.6078\n\ncat(\"Variance income Value:\", variance_value_income, \"\\n\")\n\n#&gt; Variance income Value: 8625646\n\n# The Standard Deviation\nstandard_deviation_value_age &lt;- sqrt(variance_value_age)\nstandard_deviation_value_income &lt;- sqrt(variance_value_income)\ncat(\"Standard Deviation age Value:\", standard_deviation_value_age, \"\\n\")\n\n#&gt; Standard Deviation age Value: 18.45556\n\ncat(\"Standard Deviation income Value:\", standard_deviation_value_income, \"\\n\")\n\n#&gt; Standard Deviation income Value: 2936.945\n\n# The Comparing Standard Deviation\nprint(\"Comparing standard deviations directly is most meaningful when the variables are measured in the same units and have similar scales. In the case of age and income, these two variables have different units and different scales. Therefore, comparing their standard deviations directly may not provide meaningful insights.\")\n\n#&gt; [1] \"Comparing standard deviations directly is most meaningful when the variables are measured in the same units and have similar scales. In the case of age and income, these two variables have different units and different scales. Therefore, comparing their standard deviations directly may not provide meaningful insights.\"\n\n# The Covariance\ncovariance_value &lt;- sum((age_var - mean_value_age) * (income_var - mean_value_income)) / length(age_var)\ncat(\"Covariance Value:\", covariance_value, \"\\n\")\n\n#&gt; Covariance Value: 29670.45\n\n# The Correlation\ncorrelation_value &lt;- covariance_value / (standard_deviation_value_age*standard_deviation_value_income)\ncat(\"Correlation Value:\", correlation_value, \"\\n\")\n\n#&gt; Correlation Value: 0.5473952\n\n# The Covariance or Correlation\nprint(\"The correlation coefficient's standardized scale makes it more intuitive for comparison. The fact that it ranges from -1 to 1 allows for a clear understanding of the strength and direction of the relationship.\")\n\n#&gt; [1] \"The correlation coefficient's standardized scale makes it more intuitive for comparison. The fact that it ranges from -1 to 1 allows for a clear understanding of the strength and direction of the relationship.\"\n\n# The conditional expected value, E[income|age&lt;=18]\nsubset_data1 &lt;- subset(random_vars, age &lt;= 18)\nconditional_expected_value1 &lt;- mean(subset_data1$income)\ncat(\"Conditional Expected Value of Income for age &lt;= 18:\", conditional_expected_value1, \"\\n\")\n\n#&gt; Conditional Expected Value of Income for age &lt;= 18: 389.6074\n\n# The conditional expected value, E[income|age&lt;=[18,65)]\nsubset_data2 &lt;- subset(random_vars, age &gt;= 18 & age &lt; 65)\nconditional_expected_value2 &lt;- mean(subset_data2$income)\ncat(\"Conditional Expected Value of Income for age in [18, 65):\", conditional_expected_value2, \"\\n\")\n\n#&gt; Conditional Expected Value of Income for age in [18, 65): 4685.734\n\n# The conditional expected value, E[income|age&gt;=65]\nsubset_data3 &lt;- subset(random_vars, age &gt;= 65)\nconditional_expected_value3 &lt;- mean(subset_data3$income)\ncat(\"Conditional Expected Value of Income for age &gt;= 65:\", conditional_expected_value3, \"\\n\")\n\n#&gt; Conditional Expected Value of Income for age &gt;= 65: 1777.237"
  },
  {
    "objectID": "content/01_journal/04_causality.html",
    "href": "content/01_journal/04_causality.html",
    "title": "Chap4: Causality",
    "section": "",
    "text": "# The dataset to explore the correlation between coffee consumption and nightmares, considering a potential confounding variable (stress levels)\n\n# Sample data for a group of individuals\nset.seed(456)\nindividuals &lt;- c(\"Amy\", \"Ben\", \"Chris\", \"Dana\", \"Eli\")\ncoffee_consumption &lt;- rnorm(length(individuals), mean = 3, sd = 1)\nnightmare_frequency &lt;- rnorm(length(individuals), mean = 2, sd = 1)\nstress_levels &lt;- rnorm(length(individuals), mean = 5, sd = 2)\n\n# The dataframe\nsample_data &lt;- data.frame(Individual = individuals, CoffeeConsumption = coffee_consumption, NightmareFrequency = nightmare_frequency, StressLevels = stress_levels)\n\n# The ggplot2 library\nlibrary(ggplot2)\n\n# The scatter plot to visualize the potential impact of stress levels\nggplot(sample_data, aes(x = CoffeeConsumption, y = NightmareFrequency, size = StressLevels)) +\n  geom_point() +\n  labs(title = \"Exploring Correlation: Coffee Consumption vs Nightmare Frequency\",\n       x = \"Coffee Consumption\",\n       y = \"Nightmare Frequency\",\n       size = \"Stress Levels\") +\n  theme_minimal()"
  },
  {
    "objectID": "content/01_journal/06_rct.html",
    "href": "content/01_journal/06_rct.html",
    "title": "Chap6: RCT",
    "section": "",
    "text": "1 Load the ggplot2 library\nlibrary(ggplot2)\n\n\n2 Read the customer data from the specified file path\ncustomer_data &lt;- readRDS(“C:/Users/user/OneDrive/Desktop/UNI/TUHH/SEM 3/CAUSAL DATA SC/RDS/abtest_online.rds”) customer_data\n\n\n3 Create ggplot objects to compare various metrics between chatbot conditions\ncompare_purchase_amount &lt;- ggplot(customer_data, aes(x = chatbot, y = purchase_amount, color = as.factor(chatbot))) + stat_summary(geom = “errorbar”, width = .5, fun.data = “mean_se”, fun.args = list(mult=1.96), show.legend = F) + labs(x = NULL, y = “purchase_amount”, title = “Difference in purchase amount”) + scale_x_discrete(labels = c(“Not Treated”,“Treated”))\ncompare_previous_visit &lt;- ggplot(customer_data, aes(x = chatbot, y = previous_visit, color = as.factor(chatbot))) + stat_summary(geom = “errorbar”, width = .5, fun.data = “mean_se”, fun.args = list(mult=1.96), show.legend = F) + labs(x = NULL, y = “previous_visit”, title = “Difference in previous visit”) + scale_x_discrete(labels = c(“Not Treated”,“Treated”))\ncompare_mobile_device &lt;- ggplot(customer_data, aes(x = chatbot, y = mobile_device, color = as.factor(chatbot))) + stat_summary(geom = “errorbar”, width = .5, fun.data = “mean_se”, fun.args = list(mult=1.96), show.legend = F) + labs(x = NULL, y = “mobile_device”, title = “Difference in mobile device”) + scale_x_discrete(labels = c(“Not Treated”,“Treated”))\ncompare_purchase &lt;- ggplot(customer_data, aes(x = chatbot, y = purchase, color = as.factor(chatbot))) + stat_summary(geom = “errorbar”, width = .5, fun.data = “mean_se”, fun.args = list(mult=1.96), show.legend = F) + labs(x = NULL, y = “purchase”, title = “Difference in purchase”) + scale_x_discrete(labels = c(“Not Treated”,“Treated”))\n\n\n4 Plot the comparisons\ncompare_purchase_amount compare_previous_visit compare_mobile_device compare_purchase\n\n\n\n\n\n5 Fit linear models to assess the impact of chatbot on purchase and purchase_amount\nlm1 &lt;- lm(purchase ~ chatbot, data = customer_data) summary(lm1)\nlm2 &lt;- lm(purchase_amount ~ chatbot, data = customer_data) summary(lm2)\n\n\n\n\n\n6 Fit interaction models to evaluate the joint effect of chatbot and mobile_device\nlm_mod1 &lt;- lm(purchase ~ chatbot * mobile_device, data = customer_data) summary(lm_mod1)\nlm_mod2 &lt;- lm(purchase_amount ~ chatbot * mobile_device, data = customer_data) summary(lm_mod2)\n\n\n\n\n\n7 Fit logistic regression model to assess the impact of chatbot on purchase likelihood\nlogistic_model &lt;- glm(purchase ~ chatbot, family = binomial(link = ‘logit’), data = customer_data)\n\n\n8 Display summary of the logistic regression model\nsummary(logistic_model)\ncat(“The chatbotTRUE coefficient is of particular interest. Since it has a negative estimate, it suggests that, compared to when the chatbot is not present, the presence of the chatbot is associated with a decrease in the log-odds of making a purchase. The p-value being less than 0.001 indicates that this effect is statistically significant.s"
  },
  {
    "objectID": "content/01_journal/08_did.html",
    "href": "content/01_journal/08_did.html",
    "title": "Chap8: DID",
    "section": "",
    "text": "# Libraries\nlibrary(dplyr)\nlibrary(readr)\nlibrary(lmtest)\n\n# The Dataset\ndata &lt;- readRDS('C:/Users/user/OneDrive/Desktop/UNI/TUHH/SEM 3/CAUSAL DATA SC/RDS/hospdd.rds')\n\n# 1. Calculate Mean Satisfaction for Treated and Control Hospitals Before and After the Treatment\n\n## Parameters for Analysis\nthreshold_month &lt;- 3.0\nthreshold_hospital &lt;- 18\n\n## Data Preparation\ndata$month &lt;- as.numeric(data$month)\ndata$hospital &lt;- as.numeric(data$hospital)\n\n## Data into Treated and Control Groups\ntreated_group &lt;- subset(data,hospital &lt;= 18)\ncontrol_group &lt;- subset(data,hospital &gt; 18)\n\n## Mean Difference Analysis Before Treatment\nbefore_control_mean &lt;- control_group %&gt;% \n  filter(month &lt;= threshold_month) %&gt;%\n  summarise(mean_satisfaction = mean(satis)) %&gt;%\n  pull(mean_satisfaction)\n\nbefore_treatment_mean  &lt;- treated_group %&gt;% \n  filter(month &lt;= threshold_month) %&gt;%\n  summarise(mean_satisfaction = mean(satis)) %&gt;%\n  pull(mean_satisfaction)\n\nmean_diff_before &lt;- before_treatment_mean - before_control_mean\n\n## Mean Difference Analysis After Treatment\nafter_control_mean &lt;- control_group %&gt;% \n  filter(month &gt; threshold_month) %&gt;%\n  summarise(mean_satisfaction = mean(satis)) %&gt;%\n  pull(mean_satisfaction)\n\nafter_treatment_mean &lt;- treated_group %&gt;% \n  filter(month &gt; threshold_month) %&gt;%\n  summarise(mean_satisfaction = mean(satis)) %&gt;%\n  pull(mean_satisfaction)\n\nmean_diff_after &lt;- after_treatment_mean - after_control_mean\n\n## D-in-D Analysis\nmean_diff_diff &lt;- mean_diff_after - mean_diff_before\n\n# 2. Use Linear Regression to Estimate with Group and Time Fixed Effects\n\n## Linear Regression Model\nmodel &lt;- lm(satis ~ procedure * as.factor(month) + as.factor(hospital), data)\n\n## The Regression Results\nsummary(model)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = satis ~ procedure * as.factor(month) + as.factor(hospital), \n#&gt;     data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;     Min      1Q  Median      3Q     Max \n#&gt; -3.1880 -0.4630  0.0024  0.4540  4.3181 \n#&gt; \n#&gt; Coefficients: (3 not defined because of singularities)\n#&gt;                               Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)                  3.1716566  0.0562244  56.411  &lt; 2e-16 ***\n#&gt; procedure                    0.8785070  0.0541087  16.236  &lt; 2e-16 ***\n#&gt; as.factor(month)2           -0.0096077  0.0292138  -0.329 0.742261    \n#&gt; as.factor(month)3            0.0219686  0.0292138   0.752 0.452080    \n#&gt; as.factor(month)4            0.0152441  0.0368748   0.413 0.679326    \n#&gt; as.factor(month)5           -0.0246564  0.0368748  -0.669 0.503740    \n#&gt; as.factor(month)6            0.0055796  0.0368748   0.151 0.879734    \n#&gt; as.factor(month)7           -0.0238856  0.0368748  -0.648 0.517169    \n#&gt; as.factor(hospital)2         0.4085664  0.0772468   5.289 1.26e-07 ***\n#&gt; as.factor(hospital)3         0.5336248  0.0793436   6.725 1.88e-11 ***\n#&gt; as.factor(hospital)4         0.2275102  0.0739460   3.077 0.002101 ** \n#&gt; as.factor(hospital)5        -0.1453529  0.0739460  -1.966 0.049375 *  \n#&gt; as.factor(hospital)6         0.4478634  0.0739460   6.057 1.46e-09 ***\n#&gt; as.factor(hospital)7         1.4044164  0.0714606  19.653  &lt; 2e-16 ***\n#&gt; as.factor(hospital)8         0.0718758  0.0763236   0.942 0.346365    \n#&gt; as.factor(hospital)9        -1.5185150  0.0782498 -19.406  &lt; 2e-16 ***\n#&gt; as.factor(hospital)10        1.6828446  0.0772468  21.785  &lt; 2e-16 ***\n#&gt; as.factor(hospital)11        0.2209653  0.0763236   2.895 0.003801 ** \n#&gt; as.factor(hospital)12       -0.0953034  0.0782498  -1.218 0.223287    \n#&gt; as.factor(hospital)13        0.4955931  0.0754708   6.567 5.50e-11 ***\n#&gt; as.factor(hospital)14        0.2330426  0.0793436   2.937 0.003323 ** \n#&gt; as.factor(hospital)15       -0.1444935  0.0793436  -1.821 0.068631 .  \n#&gt; as.factor(hospital)16        1.4142680  0.0772468  18.308  &lt; 2e-16 ***\n#&gt; as.factor(hospital)17        0.4235429  0.0805415   5.259 1.49e-07 ***\n#&gt; as.factor(hospital)18        0.1532761  0.0938225   1.634 0.102369    \n#&gt; as.factor(hospital)19       -0.7453017  0.0811676  -9.182  &lt; 2e-16 ***\n#&gt; as.factor(hospital)20        0.0473874  0.0791192   0.599 0.549234    \n#&gt; as.factor(hospital)21        1.1943370  0.0836287  14.281  &lt; 2e-16 ***\n#&gt; as.factor(hospital)22        0.7993153  0.0823390   9.708  &lt; 2e-16 ***\n#&gt; as.factor(hospital)23        0.7017202  0.0811676   8.645  &lt; 2e-16 ***\n#&gt; as.factor(hospital)24       -0.3081260  0.0866459  -3.556 0.000379 ***\n#&gt; as.factor(hospital)25        0.6464736  0.0927319   6.971 3.41e-12 ***\n#&gt; as.factor(hospital)26        0.2142471  0.0791192   2.708 0.006787 ** \n#&gt; as.factor(hospital)27       -0.3986544  0.0766156  -5.203 2.01e-07 ***\n#&gt; as.factor(hospital)28        0.7119953  0.0836287   8.514  &lt; 2e-16 ***\n#&gt; as.factor(hospital)29        0.2485512  0.0800987   3.103 0.001923 ** \n#&gt; as.factor(hospital)30       -0.1679220  0.0953700  -1.761 0.078324 .  \n#&gt; as.factor(hospital)31        0.5120848  0.0791192   6.472 1.03e-10 ***\n#&gt; as.factor(hospital)32       -0.3233456  0.0800987  -4.037 5.47e-05 ***\n#&gt; as.factor(hospital)33       -0.4539752  0.0791192  -5.738 9.97e-09 ***\n#&gt; as.factor(hospital)34       -0.0004123  0.0746103  -0.006 0.995591    \n#&gt; as.factor(hospital)35        0.3541110  0.0766156   4.622 3.87e-06 ***\n#&gt; as.factor(hospital)36        2.1381425  0.0773862  27.630  &lt; 2e-16 ***\n#&gt; as.factor(hospital)37        0.1404036  0.0927319   1.514 0.130049    \n#&gt; as.factor(hospital)38       -0.0868060  0.0782181  -1.110 0.267124    \n#&gt; as.factor(hospital)39       -0.0234969  0.0823390  -0.285 0.775370    \n#&gt; as.factor(hospital)40        1.1215331  0.0782181  14.339  &lt; 2e-16 ***\n#&gt; as.factor(hospital)41       -0.1497346  0.0766156  -1.954 0.050697 .  \n#&gt; as.factor(hospital)42        0.8811369  0.0850564  10.359  &lt; 2e-16 ***\n#&gt; as.factor(hospital)43       -0.7724325  0.0811676  -9.517  &lt; 2e-16 ***\n#&gt; as.factor(hospital)44        0.0344120  0.0904396   0.380 0.703588    \n#&gt; as.factor(hospital)45       -0.2137495  0.0766156  -2.790 0.005286 ** \n#&gt; as.factor(hospital)46        0.0784915  0.0823390   0.953 0.340484    \n#&gt; procedure:as.factor(month)2         NA         NA      NA       NA    \n#&gt; procedure:as.factor(month)3         NA         NA      NA       NA    \n#&gt; procedure:as.factor(month)4 -0.0750732  0.0684427  -1.097 0.272731    \n#&gt; procedure:as.factor(month)5  0.0061613  0.0684427   0.090 0.928272    \n#&gt; procedure:as.factor(month)6 -0.0531645  0.0684427  -0.777 0.437317    \n#&gt; procedure:as.factor(month)7         NA         NA      NA       NA    \n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 0.7239 on 7312 degrees of freedom\n#&gt; Multiple R-squared:  0.5334, Adjusted R-squared:  0.5299 \n#&gt; F-statistic:   152 on 55 and 7312 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "content/01_journal/10_rdd.html",
    "href": "content/01_journal/10_rdd.html",
    "title": "Chap10: RDD",
    "section": "",
    "text": "# Libraries\nlibrary(dplyr)\n\n#&gt; \n#&gt; Attaching package: 'dplyr'\n\n\n#&gt; The following objects are masked from 'package:stats':\n#&gt; \n#&gt;     filter, lag\n\n\n#&gt; The following objects are masked from 'package:base':\n#&gt; \n#&gt;     intersect, setdiff, setequal, union\n\nlibrary(ggplot2)\n\n# Data for the current campaign\ndf &lt;- readRDS('C:/Users/user/OneDrive/Desktop/UNI/TUHH/SEM 3/CAUSAL DATA SC/RDS/coupon.rds')\n\n# [1] Regression Discontinuity Design Sensitivity Analysis ----\n\n# The cut-off\nc0 &lt;- 60\n\n# Bandwidths\nbw_original &lt;- c0 + c(-5, 5)\nbw_half &lt;- c0 + c(-5, 5) / 2\nbw_double &lt;- c0 + c(-5, 5) * 2\n\n# Function to run the regression discontinuity design analysis\nrun_rdd_analysis &lt;- function(df, bw) {\n  df_bw_below &lt;- df %&gt;% filter(days_since_last &gt;= bw[1] & days_since_last &lt; c0)\n  df_bw_above &lt;- df %&gt;% filter(days_since_last &gt;= c0 & days_since_last &lt;= bw[2])\n  df_bw &lt;- bind_rows(df_bw_above, df_bw_below)\n  \n  lm_bw &lt;- lm(purchase_after ~ days_since_last_centered + coupon, df_bw)\n  \n  model_bw_below &lt;- lm(purchase_after ~ days_since_last, df_bw_below)\n  model_bw_above &lt;- lm(purchase_after ~ days_since_last, df_bw_above)\n  \n  y0 &lt;- predict(model_bw_below, tibble(days_since_last = c0))\n  y1 &lt;- predict(model_bw_above, tibble(days_since_last = c0))\n  \n  late &lt;- y1 - y0\n  return(list(LATE = late, Summary = summary(lm_bw)))\n}\n\n# Run the analysis with the original bandwidth\nLATE_original &lt;- run_rdd_analysis(df, bw_original)\n\n# Run the analysis with half the bandwidth\nLATE_half_bandwidth &lt;- run_rdd_analysis(df, bw_half)\n\n# Run the analysis with double the bandwidth\nLATE_double_bandwidth &lt;- run_rdd_analysis(df, bw_double)\n\n# The results for the original bandwidth\ncat(\"Original Bandwidth:\\n\")\n\n#&gt; Original Bandwidth:\n\ncat(\"LATE:\", LATE_original$LATE, \"\\n\")\n\n#&gt; LATE: 7.989037\n\nprint(LATE_original$Summary)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -11.4966  -2.1312  -0.0949   2.0185  10.4159 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.4242     0.3965  28.813  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.3835     0.1259   3.046  0.00251 ** \n#&gt; couponTRUE                 7.9334     0.7087  11.194  &lt; 2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.186 on 320 degrees of freedom\n#&gt; Multiple R-squared:  0.7074, Adjusted R-squared:  0.7055 \n#&gt; F-statistic: 386.8 on 2 and 320 DF,  p-value: &lt; 2.2e-16\n\n# The results for half the bandwidth\ncat(\"\\nHalf the Bandwidth:\\n\")\n\n#&gt; \n#&gt; Half the Bandwidth:\n\ncat(\"LATE:\", LATE_half_bandwidth$LATE, \"\\n\")\n\n#&gt; LATE: 7.362377\n\nprint(LATE_half_bandwidth$Summary)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -10.9680  -2.2013   0.1676   2.1516   8.2567 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)               11.6612     0.5747  20.292  &lt; 2e-16 ***\n#&gt; days_since_last_centered   0.6883     0.3219   2.138   0.0339 *  \n#&gt; couponTRUE                 7.1679     1.0172   7.047 3.87e-11 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.289 on 178 degrees of freedom\n#&gt; Multiple R-squared:  0.6622, Adjusted R-squared:  0.6584 \n#&gt; F-statistic: 174.5 on 2 and 178 DF,  p-value: &lt; 2.2e-16\n\n# The results for double the bandwidth\ncat(\"\\nDouble the Bandwidth:\\n\")\n\n#&gt; \n#&gt; Double the Bandwidth:\n\ncat(\"LATE:\", LATE_double_bandwidth$LATE, \"\\n\")\n\n#&gt; LATE: 9.513531\n\nprint(LATE_double_bandwidth$Summary)\n\n#&gt; \n#&gt; Call:\n#&gt; lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#&gt;     data = df_bw)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -12.2718  -2.0858  -0.0003   2.0275  10.6749 \n#&gt; \n#&gt; Coefficients:\n#&gt;                          Estimate Std. Error t value Pr(&gt;|t|)    \n#&gt; (Intercept)              10.61700    0.27386  38.767   &lt;2e-16 ***\n#&gt; days_since_last_centered  0.01413    0.04255   0.332     0.74    \n#&gt; couponTRUE                9.51584    0.48628  19.569   &lt;2e-16 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 3.115 on 626 degrees of freedom\n#&gt; Multiple R-squared:  0.7052, Adjusted R-squared:  0.7042 \n#&gt; F-statistic: 748.6 on 2 and 626 DF,  p-value: &lt; 2.2e-16\n\ncat(\"   ## The estimated LATE remains consistent across the three bandwidth choices, indicating a stable positive effect \n      of the coupon variable on purchase_after.\")\n\n#&gt;    ## The estimated LATE remains consistent across the three bandwidth choices, indicating a stable positive effect \n#&gt;       of the coupon variable on purchase_after.\n\ncat(\"   ## The LATE estimate is slightly smaller for the half bandwidth, suggesting a more cautious estimate.\")\n\n#&gt;    ## The LATE estimate is slightly smaller for the half bandwidth, suggesting a more cautious estimate.\n\ncat(\"   ## The LATE estimate is slightly larger for the double bandwidth, suggesting a potentially broader impact \n      on individuals farther from the cutoff point.\")\n\n#&gt;    ## The LATE estimate is slightly larger for the double bandwidth, suggesting a potentially broader impact \n#&gt;       on individuals farther from the cutoff point.\n\ncat(\"   ## Bandwidth choice influences the treatment (couponTRUE) effect estimation.\")\n\n#&gt;    ## Bandwidth choice influences the treatment (couponTRUE) effect estimation.\n\ncat(\"   ## Coefficients vary with changes in bandwidth.\")\n\n#&gt;    ## Coefficients vary with changes in bandwidth.\n\n# [2] Different Past Campaign ----\n\n# Read data for the different past campaign\ndf_shipping &lt;- readRDS('C:/Users/user/OneDrive/Desktop/UNI/TUHH/SEM 3/CAUSAL DATA SC/RDS/shipping.rds')\n\nlibrary(rddensity)\nrddd &lt;- rddensity(df_shipping$purchase_amount, c = 30)\nsummary(rddd)\n\n#&gt; \n#&gt; Manipulation testing using local polynomial density estimation.\n#&gt; \n#&gt; Number of obs =       6666\n#&gt; Model =               unrestricted\n#&gt; Kernel =              triangular\n#&gt; BW method =           estimated\n#&gt; VCE method =          jackknife\n#&gt; \n#&gt; c = 30                Left of c           Right of c          \n#&gt; Number of obs         3088                3578                \n#&gt; Eff. Number of obs    2221                1955                \n#&gt; Order est. (p)        2                   2                   \n#&gt; Order bias (q)        3                   3                   \n#&gt; BW est. (h)           22.909              20.394              \n#&gt; \n#&gt; Method                T                   P &gt; |T|             \n#&gt; Robust                5.9855              0\n\n\n#&gt; Warning in summary.CJMrddensity(rddd): There are repeated observations. Point\n#&gt; estimates and standard errors have been adjusted. Use option massPoints=FALSE\n#&gt; to suppress this feature.\n\n\n#&gt; \n#&gt; P-values of binomial tests (H0: p=0.5).\n#&gt; \n#&gt; Window Length / 2          &lt;c     &gt;=c    P&gt;|T|\n#&gt; 0.261                      20      26    0.4614\n#&gt; 0.522                      41      65    0.0250\n#&gt; 0.783                      62     107    0.0007\n#&gt; 1.043                      81     136    0.0002\n#&gt; 1.304                     100     169    0.0000\n#&gt; 1.565                     114     196    0.0000\n#&gt; 1.826                     132     227    0.0000\n#&gt; 2.087                     156     263    0.0000\n#&gt; 2.348                     173     298    0.0000\n#&gt; 2.609                     191     331    0.0000\n\ncat(\"   ## The manipulation tests reveal significant evidence of manipulation around the cut-off point (c=30)\")\n\n#&gt;    ## The manipulation tests reveal significant evidence of manipulation around the cut-off point (c=30)\n\ncat(\"   ## P-values of the robust T-statistic approach zero, indicating systematic changes in the observed\n      density of the purchase_amount variable near the cut-off.\")\n\n#&gt;    ## P-values of the robust T-statistic approach zero, indicating systematic changes in the observed\n#&gt;       density of the purchase_amount variable near the cut-off.\n\ncat(\"   ## The order of estimation and bias differs on each side, suggesting a lack of smoothness or continuity.\")\n\n#&gt;    ## The order of estimation and bias differs on each side, suggesting a lack of smoothness or continuity.\n\ncat(\"   ## The p-values of binomial tests further indicate non-random behavior around the cut-off.\")\n\n#&gt;    ## The p-values of binomial tests further indicate non-random behavior around the cut-off.\n\ncat(\"   ## The purchase_amount variable, based on the manipulation testing results, may not be suitable as a running \n      variable for an RDD with a cut-off at 30€\")\n\n#&gt;    ## The purchase_amount variable, based on the manipulation testing results, may not be suitable as a running \n#&gt;       variable for an RDD with a cut-off at 30€\n\n# Plot to confirm that purchase_amount could not be used as a running variable at 30\nggplot(df_shipping, aes(x = purchase_amount)) +\n  geom_histogram(binwidth = 5, fill = \"brown\", color = \"black\") +\n  geom_vline(xintercept = 30, color = \"blue\", linetype = \"dashed\") +\n  xlab(\"Purchase Amount (€)\") +\n  ylab(\"Number of Purchases\") +\n  theme_minimal()"
  }
]